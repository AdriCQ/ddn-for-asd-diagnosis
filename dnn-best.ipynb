{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural network for ASD classification using resting-state fMRI\n",
    "\n",
    "This notebook evaluate a deep neural network for ASD diagnosis using functiona√± time series data from brain regions of interest. The used resting-state fMRI data from the ABIDE dataset were preprocessed by the **Preprocessed Connectome Project (PCP)** using four pipelines, involving 1100 subjects from multiple international sites.\n",
    "\n",
    "### Configure the loading data\n",
    "\n",
    " The variables necessary for loading the neuroimaging data are defined. The `pipeline` and `atlas` used for preprocessing and ROIs extraction are specified. Additionally, list all neuroimaging sites available in the dataset and those that are to be included in the analysis are selected using the `sites` and `test_site` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = 'fsl'  \n",
    "rois = 'rois_ho'\n",
    "\n",
    "# List of all available neuroimaging sites in the dataset\n",
    "all_sites = [\n",
    "    'caltech', 'cmu', 'kki', 'leuven_1', 'leuven_2', 'max_mun', 'nyu', \n",
    "    'ohsu', 'olin', 'pitt', 'sbl', 'sdsu', 'stanford', 'trinity', \n",
    "    'ucla_1', 'ucla_2', 'um_1', 'um_2', 'usm', 'yale'\n",
    "]\n",
    "\n",
    "# Sites include in the analysis\n",
    "sites = all_sites\n",
    "\n",
    "# Testing site\n",
    "test_site = 'yale'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROIs data loading function\n",
    "\n",
    "Definition of the `load_rois_data(pipeline, rois, sites)` function to retrieve subject time series and diagnostic labels from each neuroimaging site in `sites`. This function reads phenotypic information from CSV files, then loads the time series data for each subject. Also handle potential issues, such as missing files or NaN values, to ensure data integrity before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_rois_data(pipeline, rois, sites):\n",
    "    \"\"\"\n",
    "    Loads time series and diagnostic labels from neuroimaging data files for the specified sites.\n",
    "    \n",
    "    Parameters:\n",
    "        pipeline (str): Preprocessing pipeline used for the data.\n",
    "        rois (str): Atlas defining regions of interest.\n",
    "        sites (list of str): List of site names to load data from.\n",
    "\n",
    "    Returns:\n",
    "        rois_time_series (dict): Contains time series data for each site.\n",
    "        rois_labels (dict): Contains diagnostic labels for each site.\n",
    "    \"\"\"\n",
    "\n",
    "    rois_time_series = {}  # Dictionary to store time series data for each site\n",
    "    rois_labels = {}  # Dictionary to store labels for each site\n",
    "\n",
    "    for site in sites:\n",
    "        # Define path for phenotypic data for the current site\n",
    "        phenotypic_path = f\"data/phenotypic_fsl/{site}/phenotypic.csv\"\n",
    "\n",
    "        try:\n",
    "            with open(phenotypic_path, 'r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                site_time_series = []  # List to store time series for each subject at the site\n",
    "                site_labels = []  # List to store labels for each subject at the site\n",
    "\n",
    "                for row in reader:\n",
    "                    file_id = row['file_id']  # Unique subject identifier\n",
    "                    dx_group = row['dx_group']  # Diagnostic group (ASD=1, Control=0)\n",
    "\n",
    "                    # Define path for the time series data file\n",
    "                    data_file_path = os.path.join(f\"data/{pipeline}/{rois}/{site}/{file_id}_{rois}.1D\")\n",
    "\n",
    "                    # Check if the data file exists\n",
    "                    if not os.path.exists(data_file_path):\n",
    "                        print(f\"File Not Found Error: Data file not found at path {data_file_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    data = np.loadtxt(data_file_path)\n",
    "\n",
    "                    # Check for NaN values and add time series to the site list\n",
    "                    if np.isnan(data).any():\n",
    "                        print(f\"Value Error: NaN value found for subject {file_id}\")\n",
    "                    else:\n",
    "                        site_time_series.append(data)\n",
    "                        site_labels.append(1 if dx_group == '1' else 0)  # Assign 1 for ASD, 0 for control\n",
    "\n",
    "                # Store loaded data for the current site in the dictionaries\n",
    "                rois_time_series[site] = site_time_series\n",
    "                rois_labels[site] = np.array(site_labels)\n",
    "                print(f\"Loaded {len(site_time_series)} subjects from site {site}.\")\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File Not Found Error: Phenotypic data not found for site {site}\")\n",
    "\n",
    "    return rois_time_series, rois_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data to be used in the analysis based on specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 subjects from site caltech.\n",
      "Loaded 3 subjects from site cmu.\n",
      "Loaded 6 subjects from site kki.\n",
      "Loaded 3 subjects from site leuven_1.\n",
      "Loaded 4 subjects from site leuven_2.\n",
      "Loaded 6 subjects from site max_mun.\n",
      "Loaded 19 subjects from site nyu.\n",
      "Loaded 3 subjects from site ohsu.\n",
      "Loaded 4 subjects from site olin.\n",
      "Loaded 6 subjects from site pitt.\n",
      "Loaded 3 subjects from site sbl.\n",
      "Loaded 4 subjects from site sdsu.\n",
      "Loaded 4 subjects from site stanford.\n",
      "Loaded 5 subjects from site trinity.\n",
      "Loaded 8 subjects from site ucla_1.\n",
      "Loaded 3 subjects from site ucla_2.\n",
      "Loaded 10 subjects from site um_1.\n",
      "Loaded 4 subjects from site um_2.\n",
      "Loaded 10 subjects from site usm.\n",
      "Loaded 14 subjects from site yale.\n"
     ]
    }
   ],
   "source": [
    "rois_time_series, rois_labels = load_rois_data(pipeline, rois, sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tangent space embedding\n",
    "\n",
    "This method allows the translation of connectivity matrices from fMRI data into a form that is compatible with Euclidean machine learning techniques while preserving the important geometric properties of the data. Is particularly useful when analyzing covariance or correlation matrices in tasks involving brain connectivity and classification of neurological conditions.\n",
    "\n",
    "The workflow involves two main steps:\n",
    "\n",
    "**Estimate the reference tangent space**: Calculate the tangent space projection based on the mean covariance matrix of a training population. This establishes the \"reference space\" against which individual test subjects can later be projected.\n",
    "\n",
    "**Project subjects onto the reference space**: Using the precomputed reference tangent space from the population, can be project the covariance matrix of a new subjects onto this space. This will yield a tangent space connectivity matrix for the subjects that aligns with those of the population.\n",
    "\n",
    "#### Create the training population\n",
    "\n",
    "To maintain a separate testing set, we exclude the `test_site` site data from the main population data used for Estimate the reference tangent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_population(time_series_data):\n",
    "    # Initialize an empty list for the population data \n",
    "    population_data = []\n",
    "\n",
    "    # Loop through the time series data\n",
    "    for item in time_series_data:\n",
    "        # Extend each item\n",
    "        population_data.extend(item)\n",
    "\n",
    "    print(f\"Total subjects in population data: {len(population_data)}\")\n",
    "    return population_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to estimating tangent space functional connectivity\n",
    "\n",
    "The `estimate_tangent_space(data)` function calculate the tangent space based on the geometric mean covariance matrix of a training population dataset. This creates a \"reference space\" that reflects the average connectivity patterns across the population.\n",
    "\n",
    "The tangent space representation of functional connectivity is a powerful tool for analyzing brain connectivity. It allows the comparison of individual functional connectivity matrices in a standardized space, computed relative to a group average matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "def estimate_tangent_space(data):\n",
    "    \"\"\"\n",
    "    Estimate the tangent space functional connectivity.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : list or ndarray\n",
    "        List or array of time series data for the training population, where each entry corresponds \n",
    "        to a subject's time series (time points x regions).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ConnectivityMeasure\n",
    "        Fitted ConnectivityMeasure object configured for tangent space transformation.\n",
    "    \"\"\"\n",
    "    # Instantiate ConnectivityMeasure for tangent space, vectorizing and discarding the diagonal\n",
    "    connectivity_measure = ConnectivityMeasure(kind='tangent', vectorize=True, discard_diagonal=True)\n",
    "\n",
    "    # Fit the measure on the population data to establish a reference tangent space\n",
    "    connectivity_measure.fit(data)\n",
    "\n",
    "    return connectivity_measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for create deep neural network models \n",
    "\n",
    "The `build_model(input_shape)` function create DNN models with the following architecture:\n",
    "\n",
    "Input Layer: Takes in the number of features from the input data.\n",
    "\n",
    "Dense Layer 1: 128 neurons, ReLU activation, with L2 regularization to reduce overfitting.\n",
    "\n",
    "Dense Layer 2: 64 neurons, ReLU activation, L2 regularization.\n",
    "\n",
    "Output Layer: A single neuron with sigmoid activation for binary classification.\n",
    "\n",
    "The model is compiled with the Adam optimizer and binary cross-entropy loss, as we aim to classify subjects into two classes. We also include accuracy as a performance metric to track model performance during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, regularizers\n",
    "\n",
    "# Define the deep neural network model architecture\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds and compiles a deep neural network model for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    - input_shape: int, the shape of the input layer, matching the number of features in the dataset\n",
    "\n",
    "    Returns:\n",
    "    - model: compiled Keras Sequential model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    # Output layer for binary classification (ASD vs. Healthy)\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Callbacks\n",
    "\n",
    "To optimize training, we set up three callbacks:\n",
    "\n",
    "**EarlyStopping:** Stops training if validation loss doesn't improve for 10 epochs, preventing overfitting and restoring the best weights.\n",
    "\n",
    "**ReduceLROnPlateau:** Reduces the learning rate by 50% when validation loss plateaus for 5 epochs, ensuring gradual and effective model convergence.\n",
    "\n",
    "**ModelCheckpoint:** Saves the model with the best validation loss to 'best_model.keras', allowing easy access to the optimal version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "# Early stopping to prevent overfitting by stopping training when validation loss stops improving\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',             # Monitor validation loss for early stopping\n",
    "    patience=10,                    # Stop training if val_loss does not improve for 10 epochs\n",
    "    restore_best_weights=True       # Restore the model weights from the epoch with the lowest val_loss\n",
    ")\n",
    "\n",
    "# Reduce learning rate when the validation loss plateaus\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',             # Monitor validation loss for learning rate reduction\n",
    "    factor=0.5,                     # Reduce learning rate by a factor of 0.5\n",
    "    patience=5,                     # Trigger after 5 epochs without improvement in val_loss\n",
    "    min_lr=1e-5                     # Set a floor on the learning rate to avoid overly small values\n",
    ")\n",
    "\n",
    "# Save the best model based on validation loss\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',             # Filename for the best model\n",
    "    monitor='val_loss',             # Monitor validation loss for checkpoint saving\n",
    "    save_best_only=True             # Only save the model when it achieves a new best val_loss\n",
    ")\n",
    "\n",
    "# Callbacks list passed to the model\n",
    "callbacks_list = [early_stopping, reduce_lr, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to adjust class balance\n",
    "\n",
    "The `adjust_class_balance(indices, labels)` function ensures equal representation of all classes by undersampling the majority class(es). This is particularly important in supervised learning, where imbalanced classes can lead to biased models. The function return shuffled list of indices representing a class-balanced subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adjust_class_balance(indices, labels):\n",
    "    \"\"\"\n",
    "    Adjusts the balance of classes by undersampling the majority class.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    indices : list or ndarray\n",
    "        Indices of the dataset.\n",
    "    labels : list or ndarray\n",
    "        Class labels corresponding to the indices.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    balanced_indices : ndarray\n",
    "        Indices of the balanced dataset.\n",
    "    \"\"\"\n",
    "    # Class labels to consider\n",
    "    CLASS_LABELS = [0, 1]\n",
    "\n",
    "    # Separate indices by class\n",
    "    class_indices = {label: [idx for idx in indices if labels[idx] == label] for label in CLASS_LABELS}\n",
    "\n",
    "    # Determine the minimum class count\n",
    "    min_class_count = min(len(indices) for indices in class_indices.values())\n",
    "\n",
    "    # Adjust class balance by undersampling the majority class\n",
    "    balanced_indices = []\n",
    "    for label, class_list in class_indices.items():\n",
    "        if len(class_list) > min_class_count:\n",
    "            sampled_indices = np.random.choice(class_list, size=min_class_count, replace=False)\n",
    "            balanced_indices.extend(sampled_indices)\n",
    "        else:\n",
    "            balanced_indices.extend(class_list)\n",
    "\n",
    "    # Shuffle the indices for randomization\n",
    "    np.random.shuffle(balanced_indices)\n",
    "    return np.array(balanced_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified cross-validation setup for model training and validation\n",
    "\n",
    "Set up stratified 10-fold cross-validation for each site (excluding `test_site`) to evaluate model performance across multiple splits. Here‚Äôs an overview of the process:\n",
    "\n",
    "**Stratified k-folds**: StratifiedKFold let to maintain the balance of classes (ASD vs. NC) across each fold, reducing potential bias.\n",
    "\n",
    "**Fold processing**: For each site, 10 training and validation folds are generated, and indices are stored in the `train_indices` and `val_indices` dictionaries. To ensure class balance after combining all group folds for training data the majority class in each site is undersampling.\n",
    "\n",
    "**Class balance checks**: For each fold, the balance of ASD and NC samples is shown to confirm each split maintains similar distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=4.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m site_val_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Loop through each fold in the stratified split\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(features, labels)):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing fold #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for site `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msite\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m     train_idx \u001b[38;5;241m=\u001b[39m adjust_class_balance(train_idx, labels)\n",
      "File \u001b[1;32mc:\\Users\\dcq\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\model_selection\\_split.py:409\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    407\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         (\n\u001b[0;32m    411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    414\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=4."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Number of cross-validation folds\n",
    "n_folds = 10\n",
    "\n",
    "# Dictionaries for save the training and validation indices\n",
    "train_indices = {}\n",
    "val_indices = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation for each site, excluding 'test_site' for testing\n",
    "for site in sites:\n",
    "    if site == test_site:\n",
    "        continue\n",
    "\n",
    "    features = rois_time_series[site]\n",
    "    labels = rois_labels[site]\n",
    "\n",
    "    # Initialize StratifiedKFold with shuffle to ensure data randomization\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "    \n",
    "    site_train_indices = []\n",
    "    site_val_indices = []\n",
    "\n",
    "    # Loop through each fold in the stratified split\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(features, labels)):\n",
    "        print(f\"Processing fold #{fold} for site `{site}`\")\n",
    "      \n",
    "        train_idx = adjust_class_balance(train_idx, labels)\n",
    "        val_idx = adjust_class_balance(val_idx, labels)\n",
    "\n",
    "        # Append training and validation indices for each fold\n",
    "        site_train_indices.append(np.array(train_idx))\n",
    "        site_val_indices.append(np.array(val_idx))\n",
    "        \n",
    "        # Print class distribution for training and validation sets for each fold\n",
    "        print(f\"Balance of classes in training -> ASD: {np.count_nonzero(labels[site_train_indices[fold]] == 1)} and TC: {np.count_nonzero(labels[site_train_indices[fold]] == 0)}\")\n",
    "        \n",
    "        print(f\"Balance of classes in validation -> ASD: {np.count_nonzero(labels[site_val_indices[fold]] == 1)} and TC: {np.count_nonzero(labels[site_val_indices[fold]] == 0)}\")\n",
    "\n",
    "    # Store indices for each fold in the dictionaries\n",
    "    train_indices[site] = site_train_indices\n",
    "    val_indices[site] = site_val_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function to calculate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_prob):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    \n",
    "    return accuracy, sensitivity, precision, specificity, auc, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(split, dataset_type, accuracy, sensitivity, precision, specificity, auc, cm):\n",
    "    print(f\"{dataset_type.capitalize()} Metrics for split {split + 1}:\")\n",
    "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  Sensitivity (Recall): {sensitivity * 100:.2f}%\")\n",
    "    print(f\"  Precision: {precision * 100:.2f}%\")\n",
    "    print(f\"  Specificity: {specificity * 100:.2f}%\")\n",
    "    print(f\"  AUC-ROC Score: {auc * 100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving tangent spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split 1 ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'caltech'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m site \u001b[38;5;241m==\u001b[39m test_site:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m X_train_time_series\u001b[38;5;241m.\u001b[39mextend([rois_time_series[site][idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43msite\u001b[49m\u001b[43m]\u001b[49m[split]])\n\u001b[0;32m     15\u001b[0m y_train\u001b[38;5;241m.\u001b[39mextend([rois_labels[site][idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m train_indices[site][split]])\n\u001b[0;32m     16\u001b[0m X_val_time_series\u001b[38;5;241m.\u001b[39mextend([rois_time_series[site][idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m val_indices[site][split]])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'caltech'"
     ]
    }
   ],
   "source": [
    "connectivity_list = []\n",
    "\n",
    "# Cross-validation across all splits\n",
    "for split in range(n_folds):\n",
    "    print(f\"\\n--- Split {split + 1} ---\")\n",
    "\n",
    "    # Aggregate training and validation data across all sites\n",
    "    X_train_time_series, X_val_time_series = [], []\n",
    "    y_train, y_val = [], []\n",
    "\n",
    "    for site in sites:\n",
    "        if site == test_site:\n",
    "            continue\n",
    "        X_train_time_series.extend([rois_time_series[site][idx] for idx in train_indices[site][split]])\n",
    "        y_train.extend([rois_labels[site][idx] for idx in train_indices[site][split]])\n",
    "        X_val_time_series.extend([rois_time_series[site][idx] for idx in val_indices[site][split]])\n",
    "        y_val.extend([rois_labels[site][idx] for idx in val_indices[site][split]])\n",
    "\n",
    "    # Prepare tangent space for feature extraction\n",
    "    connectivity_m = estimate_tangent_space(X_train_time_series)\n",
    "    connectivity_list.append(connectivity_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split 1 ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'caltech'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m site \u001b[38;5;241m==\u001b[39m test_site:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X_train_time_series\u001b[38;5;241m.\u001b[39mextend([rois_time_series[site][idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43msite\u001b[49m\u001b[43m]\u001b[49m[split]])\n\u001b[0;32m     19\u001b[0m y_train\u001b[38;5;241m.\u001b[39mextend([rois_labels[site][idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m train_indices[site][split]])\n\u001b[0;32m     20\u001b[0m X_val_time_series\u001b[38;5;241m.\u001b[39mextend([rois_time_series[site][idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m val_indices[site][split]])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'caltech'"
     ]
    }
   ],
   "source": [
    "# Initialize accumulators for metrics\n",
    "metrics = {\n",
    "    \"validation\": {\"accuracy\": 0, \"sensitivity\": 0, \"precision\": 0, \"specificity\": 0, \"auc\": 0},\n",
    "    \"test\": {\"accuracy\": 0, \"sensitivity\": 0, \"precision\": 0, \"specificity\": 0, \"auc\": 0}\n",
    "}\n",
    "\n",
    "# Cross-validation across all splits\n",
    "for split in range(n_folds):\n",
    "    print(f\"\\n--- Split {split + 1} ---\")\n",
    "\n",
    "    # Aggregate training and validation data across all sites\n",
    "    X_train_time_series, X_val_time_series = [], []\n",
    "    y_train, y_val = [], []\n",
    "\n",
    "    for site in sites:\n",
    "        if site == test_site:\n",
    "            continue\n",
    "        X_train_time_series.extend([rois_time_series[site][idx] for idx in train_indices[site][split]])\n",
    "        y_train.extend([rois_labels[site][idx] for idx in train_indices[site][split]])\n",
    "        X_val_time_series.extend([rois_time_series[site][idx] for idx in val_indices[site][split]])\n",
    "        y_val.extend([rois_labels[site][idx] for idx in val_indices[site][split]])\n",
    "\n",
    "    # Prepare tangent space for feature extraction\n",
    "    X_train = connectivity_list[split].transform(X_train_time_series)\n",
    "    X_val = connectivity_list[split].transform(X_val_time_series)\n",
    "    X_test = connectivity_list[split].transform(rois_time_series[test_site])\n",
    "    y_test = rois_labels[test_site]\n",
    "\n",
    "    X_train, X_val, X_test = map(np.array, [X_train, X_val, X_test])\n",
    "    y_train, y_val, y_test = map(np.array, [y_train, y_val, y_test])\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print(f\"Training set shape: {X_train.shape}, class balance: ASD={np.sum(y_train == 1)}, TC={np.sum(y_train == 0)}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}, class balance: ASD={np.sum(y_val == 1)}, TC={np.sum(y_val == 0)}\")\n",
    "    print(f\"Test set shape: {X_test.shape}, class balance: ASD={np.sum(y_test == 1)}, TC={np.sum(y_test == 0)}\")\n",
    "\n",
    "    # Build and train the model\n",
    "    dnn = build_model(X_train.shape[1])\n",
    "    history = dnn.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    validation_pred_prob = dnn.predict(X_val).ravel()\n",
    "    validation_pred = (validation_pred_prob > 0.5).astype(int)\n",
    "    acc, sens, prec, spec, auc, cm = calculate_metrics(y_val, validation_pred, validation_pred_prob)\n",
    "    metrics[\"validation\"][\"accuracy\"] += acc\n",
    "    metrics[\"validation\"][\"sensitivity\"] += sens\n",
    "    metrics[\"validation\"][\"precision\"] += prec\n",
    "    metrics[\"validation\"][\"specificity\"] += spec\n",
    "    metrics[\"validation\"][\"auc\"] += auc\n",
    "    print_metrics(split, \"validation\", acc, sens, prec, spec, auc, cm)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_pred_prob = dnn.predict(X_test).ravel()\n",
    "    test_pred = (test_pred_prob > 0.5).astype(int)\n",
    "    acc, sens, prec, spec, auc, cm = calculate_metrics(y_test, test_pred, test_pred_prob)\n",
    "    metrics[\"test\"][\"accuracy\"] += acc\n",
    "    metrics[\"test\"][\"sensitivity\"] += sens\n",
    "    metrics[\"test\"][\"precision\"] += prec\n",
    "    metrics[\"test\"][\"specificity\"] += spec\n",
    "    metrics[\"test\"][\"auc\"] += auc\n",
    "    print_metrics(split, \"test\", acc, sens, prec, spec, auc, cm)\n",
    "\n",
    "# Print mean metrics\n",
    "for dataset in metrics:\n",
    "    print(f\"\\n--- Mean {dataset.capitalize()} Metrics Across All Splits ---\")\n",
    "    for metric, value in metrics[dataset].items():\n",
    "        print(f\"{metric.capitalize()}: {(value / n_folds) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing with the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tangent space estimated.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Aggregate all training data\n",
    "X_train_time_series, y_data = [], []\n",
    "\n",
    "for site in sites:\n",
    "    if site == test_site:\n",
    "        continue\n",
    "\n",
    "    X_train_time_series.extend(rois_time_series[site])\n",
    "    y_data.extend(rois_labels[site])\n",
    "\n",
    "# Prepare tangent space for feature extraction\n",
    "connectivity_m = estimate_tangent_space(X_train_time_series)\n",
    "print(\"Tangent space estimated.\")\n",
    "\n",
    "# Transform data into feature vectors\n",
    "X_data = connectivity_m.transform(X_train_time_series)\n",
    "X_test = connectivity_m.transform(rois_time_series[test_site])\n",
    "y_test = rois_labels[test_site]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (87, 6105), class balance: ASD=44, TC=43\n",
      "Validation set shape: (22, 6105), class balance: ASD=11, TC=11\n",
      "Test set shape: (14, 6105), class balance: ASD=7, TC=7\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 2s 250ms/step - loss: 1.0464 - accuracy: 0.4253 - val_loss: 1.0285 - val_accuracy: 0.5455 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.9664 - accuracy: 0.7586 - val_loss: 1.0209 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.9139 - accuracy: 0.8506 - val_loss: 1.0136 - val_accuracy: 0.5909 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.8632 - accuracy: 0.9195 - val_loss: 1.0068 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.7842 - accuracy: 0.9885 - val_loss: 1.0010 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.7131 - accuracy: 0.9655 - val_loss: 0.9954 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6576 - accuracy: 0.9885 - val_loss: 0.9903 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5917 - accuracy: 1.0000 - val_loss: 0.9845 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5431 - accuracy: 1.0000 - val_loss: 0.9792 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4904 - accuracy: 1.0000 - val_loss: 0.9744 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4529 - accuracy: 1.0000 - val_loss: 0.9686 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4311 - accuracy: 1.0000 - val_loss: 0.9627 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3855 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.3769 - accuracy: 1.0000 - val_loss: 0.9508 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.3600 - accuracy: 1.0000 - val_loss: 0.9447 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.3362 - accuracy: 1.0000 - val_loss: 0.9386 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.3340 - accuracy: 1.0000 - val_loss: 0.9321 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.3199 - accuracy: 1.0000 - val_loss: 0.9251 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.3076 - accuracy: 1.0000 - val_loss: 0.9178 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.2987 - accuracy: 1.0000 - val_loss: 0.9099 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.2961 - accuracy: 1.0000 - val_loss: 0.9018 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.2875 - accuracy: 1.0000 - val_loss: 0.8939 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2836 - accuracy: 1.0000 - val_loss: 0.8858 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.2748 - accuracy: 1.0000 - val_loss: 0.8784 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.2673 - accuracy: 1.0000 - val_loss: 0.8710 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.2589 - accuracy: 1.0000 - val_loss: 0.8638 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.2492 - accuracy: 1.0000 - val_loss: 0.8570 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.2428 - accuracy: 1.0000 - val_loss: 0.8505 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.2362 - accuracy: 1.0000 - val_loss: 0.8440 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.2317 - accuracy: 1.0000 - val_loss: 0.8380 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.2229 - accuracy: 1.0000 - val_loss: 0.8322 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.2191 - accuracy: 1.0000 - val_loss: 0.8270 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2103 - accuracy: 1.0000 - val_loss: 0.8218 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2039 - accuracy: 1.0000 - val_loss: 0.8165 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1998 - accuracy: 1.0000 - val_loss: 0.8108 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1946 - accuracy: 1.0000 - val_loss: 0.8052 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.1895 - accuracy: 1.0000 - val_loss: 0.7996 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.1874 - accuracy: 1.0000 - val_loss: 0.7948 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.1813 - accuracy: 1.0000 - val_loss: 0.7902 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1719 - accuracy: 1.0000 - val_loss: 0.7854 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 247ms/step - loss: 0.1729 - accuracy: 1.0000 - val_loss: 0.7805 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.1656 - accuracy: 1.0000 - val_loss: 0.7754 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.1648 - accuracy: 1.0000 - val_loss: 0.7706 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.1584 - accuracy: 1.0000 - val_loss: 0.7653 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.1538 - accuracy: 1.0000 - val_loss: 0.7601 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.1465 - accuracy: 1.0000 - val_loss: 0.7557 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1461 - accuracy: 1.0000 - val_loss: 0.7515 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.1416 - accuracy: 1.0000 - val_loss: 0.7476 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 230ms/step - loss: 0.1383 - accuracy: 1.0000 - val_loss: 0.7436 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.1320 - accuracy: 1.0000 - val_loss: 0.7396 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.1366 - accuracy: 1.0000 - val_loss: 0.7354 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.1292 - accuracy: 1.0000 - val_loss: 0.7314 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.1242 - accuracy: 1.0000 - val_loss: 0.7275 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1222 - accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.1191 - accuracy: 1.0000 - val_loss: 0.7191 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.1166 - accuracy: 1.0000 - val_loss: 0.7140 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.7092 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 230ms/step - loss: 0.1099 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.1065 - accuracy: 1.0000 - val_loss: 0.7008 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.1081 - accuracy: 1.0000 - val_loss: 0.6972 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.6942 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.1031 - accuracy: 1.0000 - val_loss: 0.6918 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0993 - accuracy: 1.0000 - val_loss: 0.6901 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0969 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0935 - accuracy: 1.0000 - val_loss: 0.6859 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.0918 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.0922 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0869 - accuracy: 1.0000 - val_loss: 0.6848 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0858 - accuracy: 1.0000 - val_loss: 0.6846 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0843 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.0795 - accuracy: 1.0000 - val_loss: 0.6802 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0789 - accuracy: 1.0000 - val_loss: 0.6783 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.6763 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 235ms/step - loss: 0.0751 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.0755 - accuracy: 1.0000 - val_loss: 0.6733 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0761 - accuracy: 1.0000 - val_loss: 0.6712 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.0715 - accuracy: 1.0000 - val_loss: 0.6698 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0698 - accuracy: 1.0000 - val_loss: 0.6683 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0698 - accuracy: 1.0000 - val_loss: 0.6659 - val_accuracy: 0.6364 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0696 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 1s 276ms/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0649 - accuracy: 1.0000 - val_loss: 0.6584 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.6522 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0605 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0619 - accuracy: 1.0000 - val_loss: 0.6496 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0598 - accuracy: 1.0000 - val_loss: 0.6485 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0601 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0585 - accuracy: 1.0000 - val_loss: 0.6478 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.6486 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0598 - accuracy: 1.0000 - val_loss: 0.6476 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0580 - accuracy: 1.0000 - val_loss: 0.6462 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0558 - accuracy: 1.0000 - val_loss: 0.6457 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.6456 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.6458 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.6459 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Training Metrics for split 2:\n",
      "  Accuracy: 100.00%\n",
      "  Sensitivity (Recall): 100.00%\n",
      "  Precision: 100.00%\n",
      "  Specificity: 100.00%\n",
      "  AUC-ROC Score: 100.00%\n",
      "  Confusion Matrix:\n",
      "[[43  0]\n",
      " [ 0 44]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Validation Metrics for split 2:\n",
      "  Accuracy: 68.18%\n",
      "  Sensitivity (Recall): 72.73%\n",
      "  Precision: 66.67%\n",
      "  Specificity: 63.64%\n",
      "  AUC-ROC Score: 75.21%\n",
      "  Confusion Matrix:\n",
      "[[7 4]\n",
      " [3 8]]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Test Metrics for split 2:\n",
      "  Accuracy: 64.29%\n",
      "  Sensitivity (Recall): 28.57%\n",
      "  Precision: 100.00%\n",
      "  Specificity: 100.00%\n",
      "  AUC-ROC Score: 73.47%\n",
      "  Confusion Matrix:\n",
      "[[7 0]\n",
      " [5 2]]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, stratify=y_data, test_size=0.2)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "X_train, X_val, X_test = map(np.array, [X_train, X_val, X_test])\n",
    "y_train, y_val, y_test = map(np.array, [y_train, y_val, y_test])\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Training set shape: {X_train.shape}, class balance: ASD={np.sum(y_train == 1)}, TC={np.sum(y_train == 0)}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, class balance: ASD={np.sum(y_val == 1)}, TC={np.sum(y_val == 0)}\")\n",
    "print(f\"Test set shape: {X_test.shape}, class balance: ASD={np.sum(y_test == 1)}, TC={np.sum(y_test == 0)}\")\n",
    "\n",
    "# Build and train the model\n",
    "dnn = build_model(X_train.shape[1])\n",
    "history = dnn.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate on training set\n",
    "train_pred_prob = dnn.predict(X_train).ravel()\n",
    "train_pred = (train_pred_prob > 0.5).astype(int)\n",
    "acc, sens, prec, spec, auc, cm = calculate_metrics(y_train, train_pred, train_pred_prob)\n",
    "print_metrics(1, \"training\", acc, sens, prec, spec, auc, cm)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_pred_prob = dnn.predict(X_val).ravel()\n",
    "val_pred = (val_pred_prob > 0.5).astype(int)\n",
    "acc, sens, prec, spec, auc, cm = calculate_metrics(y_val, val_pred, val_pred_prob)\n",
    "print_metrics(1, \"validation\", acc, sens, prec, spec, auc, cm)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_pred_prob = dnn.predict(X_test).ravel()\n",
    "test_pred = (test_pred_prob > 0.5).astype(int)\n",
    "acc, sens, prec, spec, auc, cm = calculate_metrics(y_test, test_pred, test_pred_prob)\n",
    "print_metrics(1, \"test\", acc, sens, prec, spec, auc, cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
